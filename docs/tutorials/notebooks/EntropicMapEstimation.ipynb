{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f4541285-4a4e-48a9-9055-305dd7b4de23",
      "metadata": {
        "id": "f4541285-4a4e-48a9-9055-305dd7b4de23"
      },
      "source": [
        "# Entropic estimation of optimal transport maps"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ecfe5064-6a6f-4747-8af9-a81a300b77f9",
      "metadata": {
        "id": "ecfe5064-6a6f-4747-8af9-a81a300b77f9"
      },
      "source": [
        "In this tutorial, we show some of the visual and statistical aspects of the entropic estimator of transport map presented in {cite}`pooladian:21`. To do so, we will use its implementation at {class}`problems.linear.potentials.EntropicPotentials.transport`.\n",
        "\n",
        "## Mathematical Formalism\n",
        "\n",
        "{cite}`pooladian:21` introduces the **barycentric projection on the optimal entropic plan** as an estimator of the optimal transport map.\n",
        "\n",
        "Formally, for $P$ a source distribution and $Q$ a target distribution, with $T_0$ the optimal transport map between these two distributions. We define $\\pi_\\epsilon$ the optimal entropic plan between $P$ and $Q$, and $(f_\\epsilon, g_\\epsilon)$ the optimal entropic potentials. The barycentric projection is defined as \n",
        "$ T_\\epsilon =  \\int y d\\pi_\\epsilon^x(y) = \\mathbb{E}_{\\pi_\\epsilon}[Y | X = x]$. \n",
        "\n",
        "Where $d\\pi_\\epsilon^x(\\cdot) = e^{(f_\\epsilon(x) + g_\\epsilon(\\cdot) - ||x-\\cdot||^2)/ϵ} dQ$\n",
        "\n",
        "In some cases, such as the two sample case where both source and target are histograms, we can get a closed form, computable in $O(n)$ time given that we have the training samples evaluations of $g_\\epsilon$. \n",
        "\n",
        "However, in `proposition 2` of {cite}`pooladian:21`, it is proven that in the general case, it is equivalent to the following quantity : $T_\\epsilon = (Id − ∇f_\\epsilon)$. \n",
        "\n",
        "This is what **OTT** does as it computes the optimal entropic potentials using the sinkhorn solver and use the latter formulation to build the estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qeaqoD0gpu3h",
      "metadata": {
        "id": "qeaqoD0gpu3h"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "qXK-hABYZ_dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXK-hABYZ_dd",
        "outputId": "9c4c6dc6-fe44-4479-9330-9bd17d6aaaaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.2/142.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ott-jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    !pip install -q git+https://github.com/ott-jax/ott@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b98269a7-aee0-4987-a01b-3270c67121f2",
      "metadata": {
        "id": "b98269a7-aee0-4987-a01b-3270c67121f2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ott.geometry import pointcloud\n",
        "from ott.problems.linear import linear_problem\n",
        "from ott.solvers.linear import sinkhorn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CualQQyqiHLF",
      "metadata": {
        "id": "CualQQyqiHLF"
      },
      "source": [
        "## Two-samples estimates\n",
        "\n",
        "We consider the case where we are given two samples set from different distribution (opposed as the case where none or only one is samples).\n",
        "\n",
        "In order to know the optimal transport maps and be able to compute errors, we fix the source distribution so that $P = \\text{Uniform}([-1, 1]^d)$. \n",
        "\n",
        "Therefore for any function $f : [-1, 1]^d → \\mathbb{R}^{d'}$, and $X_n \\sim P$, $Y_n := f(X_n) \\sim Q$ with  $dQ = f dλ$\n",
        "\n",
        "This ensures that we obtain valid erros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "df17587c-1825-4f15-8763-0450ee511292",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df17587c-1825-4f15-8763-0450ee511292",
        "outputId": "e49834ad-2124-4793-d866-275fcb04507a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "rng, *rngs = jax.random.split(rng, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0f1a06a9",
      "metadata": {
        "id": "0f1a06a9"
      },
      "outputs": [],
      "source": [
        "def get_estimated_transport_map(x : jax.Array, y : jax.Array, epsilon : float) -> Callable:\n",
        "    geom = pointcloud.PointCloud(x=x, y=y, epsilon = epsilon)\n",
        "    solver_output = sinkhorn.solve(geom)\n",
        "\n",
        "    if isinstance(solver_output, sinkhorn.SinkhornOutput):\n",
        "      #Always verified as sinkhorn.solve rank argument is defaulted to -1\n",
        "      #it is only to avoid trigerring linters\n",
        "      return solver_output.to_dual_potentials().transport\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9f285d-0f54-4da2-be4a-abf8ccd0b3f4",
      "metadata": {
        "id": "2f9f285d-0f54-4da2-be4a-abf8ccd0b3f4"
      },
      "source": [
        "### 2-dimensional case"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce0e934-dfa0-4b0c-bdd2-45c63373e82b",
      "metadata": {
        "id": "6ce0e934-dfa0-4b0c-bdd2-45c63373e82b"
      },
      "source": [
        "We visualize the outputs of our estimator implementation in 2 dimensions, for a uniform source distribution and two simple explicit transports."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CGHQR8nNo_wD",
      "metadata": {
        "id": "CGHQR8nNo_wD"
      },
      "source": [
        "For the first example, we take $T_0 : x \\in [-1, 1]^d → e^x$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ae5c32bf-e81d-43fd-b1ac-06b579b20db1",
      "metadata": {
        "id": "ae5c32bf-e81d-43fd-b1ac-06b579b20db1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "n, dim = 1000, 2\n",
        "x = jax.random.uniform(rngs[0], (n, dim), minval=-1, maxval=1)\n",
        "y = jnp.exp(x)\n",
        "\n",
        "m = 1000\n",
        "x_ = jax.random.uniform(rngs[1], (m, dim), minval=-1, maxval=1)\n",
        "y_ = jnp.exp(x_)\n",
        "\n",
        "estimated_map = get_estimated_transport_map(x, y, 0.05)\n",
        "y_hat = estimated_map(x_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c974e76d-6db4-44a5-8db8-93afe08f6a6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "c974e76d-6db4-44a5-8db8-93afe08f6a6f",
        "outputId": "c60bd99f-d139-4432-8dd6-819a0f81ecb8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_[:, 0], y_[:, 1], marker=\"+\", label=\"Sampled from $Q$\")\n",
        "plt.scatter(\n",
        "    y_hat[:, 0], y_hat[:, 1], marker=\"+\", label=\"Sampled from $\\hat{T}_\\epsilon\\# P$\"\n",
        ")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LQPmH5UFoe5I",
      "metadata": {
        "id": "LQPmH5UFoe5I"
      },
      "source": [
        "For the second example, we take $T_0 : x \\in [-1, 1]^d → 3x^2 \\cdot \\text{sgn}(x)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d59ad32e-e0fe-4103-a852-e994b7f075b2",
      "metadata": {
        "id": "d59ad32e-e0fe-4103-a852-e994b7f075b2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "n, dim = 1000, 2\n",
        "x = jax.random.uniform(rngs[2], (n, dim), minval=-1, maxval=1)\n",
        "y = 3 * x ** 2 * jnp.sign(x)\n",
        "\n",
        "m = 1000\n",
        "x_ = jax.random.uniform(rngs[3], (m, dim), minval=-1, maxval=1)\n",
        "y_ = 3 * x_ ** 2 * jnp.sign(x_)\n",
        "estimated_map = get_estimated_transport_map(x, y, 0.05)\n",
        "y_hat = estimated_map(x_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "875182e6-3561-48a0-a250-6cd94a4e5f6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "875182e6-3561-48a0-a250-6cd94a4e5f6e",
        "outputId": "7865a18e-0b71-486d-88a2-605e9b735056",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_[:, 0], y_[:, 1], marker=\"+\", label=\"Sampled from $Q$\")\n",
        "plt.scatter(\n",
        "    y_hat[:, 0], y_hat[:, 1], marker=\"+\", label=\"Sampled from $\\hat{T}_\\epsilon\\# P$\"\n",
        ")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01fd9443-3967-41cf-8d53-ac28078d05d4",
      "metadata": {
        "id": "01fd9443-3967-41cf-8d53-ac28078d05d4"
      },
      "source": [
        "## MSE according to the number of samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d6625a-3184-43fe-8038-51a1d6876b18",
      "metadata": {
        "id": "d2d6625a-3184-43fe-8038-51a1d6876b18"
      },
      "source": [
        "We show the MSE error (Monte Carlo estimation) made by the estimator for different numbers of samples and different dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f70bcfbe-0d71-44ae-83be-9a1f45db5dd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "f70bcfbe-0d71-44ae-83be-9a1f45db5dd5",
        "outputId": "012ced7f-79f6-4b78-d767-1cc891c01e75",
        "tags": []
      },
      "outputs": [],
      "source": [
        "M = 10  # number of MC iterations\n",
        "m = 2000  # number of samples in MSE estimation\n",
        "dims = [2, 3, 5, 10]\n",
        "colors = [\"r\", \"g\", \"b\", \"y\", \"m\", \"c\", \"k\", \"w\"]\n",
        "n_samples = np.logspace(1, 3, 9)\n",
        "eps = 0.05\n",
        "\n",
        "\n",
        "for dim, color in zip(dims, colors):\n",
        "    errors = np.zeros((M, 9))\n",
        "    for i in range(M):\n",
        "        for j, n in enumerate(n_samples):\n",
        "            rng = jax.random.PRNGKey(i)\n",
        "            n = int(n)\n",
        "            x = jax.random.uniform(rng, (n, dim), minval=-1, maxval=1)\n",
        "            y = jnp.exp(x)\n",
        "            x_ = jax.random.uniform(rng, (m, dim), minval=-1, maxval=1)\n",
        "            y_ = jnp.exp(x_)\n",
        "            estimated_map = get_estimated_transport_map(x, y, 0.05)\n",
        "            y_hat = estimated_map(x_)\n",
        "            errors[i, j] = (jnp.linalg.norm(y_ - y_hat) ** 2 / m).item()\n",
        "    plt.plot(n_samples, np.mean(errors, axis=0), label=dim, color=color)\n",
        "    plt.fill_between(\n",
        "        n_samples,\n",
        "        np.mean(errors, axis=0) - np.std(errors, axis=0),\n",
        "        np.mean(errors, axis=0) + np.std(errors, axis=0),\n",
        "        alpha=0.2,\n",
        "        color=color,\n",
        "    )\n",
        "\n",
        "\n",
        "plt.xlabel\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"n samples\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend(title=\"Dimension\")\n",
        "plt.title(\"MSE means ± one std\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fF23gcaFg1kj",
      "metadata": {
        "id": "fF23gcaFg1kj"
      },
      "source": [
        "## MSE as a function of epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pNkpOEtzomPx",
      "metadata": {
        "id": "pNkpOEtzomPx"
      },
      "source": [
        "We show the MSE error (Monte Carlo estimation) made by the estimator for different values of $\\epsilon$, for $n=1000$ and $d=10$. \n",
        "\n",
        "We consider $T_0 : x → e^x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0oyuVeB6g0jT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "0oyuVeB6g0jT",
        "outputId": "5ab6876c-0351-4321-886c-83a1ca72b912"
      },
      "outputs": [],
      "source": [
        "M = 10  # number of MC iterations\n",
        "m = 100  # number of samples in MSE estimation\n",
        "dim = 10\n",
        "n = 1000\n",
        "epsilons = np.logspace(-2, 0, 15)\n",
        "errors = np.zeros((M, 15))\n",
        "\n",
        "for j, eps in enumerate(epsilons):\n",
        "    for i in range(M):\n",
        "        rng = jax.random.PRNGKey(i)\n",
        "        n = int(n)\n",
        "        x = jax.random.uniform(rng, (n, dim), minval=-1, maxval=1)\n",
        "        y = jnp.exp(x)\n",
        "        x_ = jax.random.uniform(rng, (m, dim), minval=-1, maxval=1)\n",
        "        y_ = jnp.exp(x_)\n",
        "        estimated_map = get_estimated_transport_map(x, y, eps)\n",
        "        y_hat = estimated_map(x_)\n",
        "        errors[i, j] = (jnp.linalg.norm(y_ - y_hat) ** 2 / m).item()\n",
        "plt.plot(epsilons, np.mean(errors, axis=0))\n",
        "plt.fill_between(\n",
        "    epsilons,\n",
        "    np.mean(errors, axis=0) - np.std(errors, axis=0),\n",
        "    np.mean(errors, axis=0) + np.std(errors, axis=0),\n",
        "    alpha=0.2,\n",
        ")\n",
        "\n",
        "\n",
        "plt.xlabel\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epsilon\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.title(\"MSE means ± one std\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-Ao37StpYXPv",
      "metadata": {
        "id": "-Ao37StpYXPv"
      },
      "source": [
        "# Benchmark on some common benchmark datasets\n",
        "\n",
        "The difficulties for benchmarking Optimal Transport Map comes from the fact that the exact solution is not known except for synthetic data. \n",
        "In the following lines, we are going to give visual examples of the matching obtained using the estimated transport map."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QGuF3aKDqZD8",
      "metadata": {
        "id": "QGuF3aKDqZD8"
      },
      "source": [
        "We first build a normal sample generator, which will be our source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "JZC4IGipbRlI",
      "metadata": {
        "id": "JZC4IGipbRlI"
      },
      "outputs": [],
      "source": [
        "def gaussian_generator(n_samples: int, noise: float, key: int) -> jax.Array:\n",
        "    assert noise < 1.0, \"The noise is larger than source distribution\"\n",
        "    return (\n",
        "        jax.random.normal(key=key, shape=(n_samples, 2))\n",
        "        + jax.random.normal(key=key, shape=(n_samples, 2)) * noise\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q3p6bf_Mqeqj",
      "metadata": {
        "id": "Q3p6bf_Mqeqj"
      },
      "source": [
        "Then we add generator of synthetic data that accepts the usual synthetic data generators of scikit-learn and ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "mPYfD_yCaDh9",
      "metadata": {
        "id": "mPYfD_yCaDh9"
      },
      "outputs": [],
      "source": [
        "class SyntheticDataGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sklearn_generator: Callable = gaussian_generator,\n",
        "        noise: float = 0.1,\n",
        "        seed: int = 1,\n",
        "        scale: float = 1,\n",
        "        loc: jax.Array = jnp.array([0, 0]),\n",
        "    ):\n",
        "        self.sklearn_generator = sklearn_generator\n",
        "        self.noise = noise\n",
        "        self.key_gen = jax.random.PRNGKey(seed)\n",
        "        self.scale = scale\n",
        "        self.loc = loc\n",
        "\n",
        "    def __get_samples(self, n_samples: int = 100) -> np.ndarray:\n",
        "        self.key_gen, key_ = jax.random.split(self.key_gen)\n",
        "        try:\n",
        "            X = self.sklearn_generator(n_samples, noise=self.noise, key=key_)\n",
        "        except TypeError:\n",
        "            X, _ = self.sklearn_generator(\n",
        "                n_samples, noise=self.noise, random_state=key_.tolist()[0]\n",
        "            )\n",
        "            X = X[:, [0, 2]]\n",
        "\n",
        "        return self.loc + self.scale * X\n",
        "\n",
        "    def __call__(self, n_samples: int = 100):\n",
        "        return self.__get_samples(n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bXh0WxqTqqNg",
      "metadata": {
        "id": "bXh0WxqTqqNg"
      },
      "source": [
        "We will look at different target distribution : the S one and the swiss roll one.\n",
        "We adapted the scale in order to have expressive plots later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cUD5pWoSbMVN",
      "metadata": {
        "id": "cUD5pWoSbMVN"
      },
      "outputs": [],
      "source": [
        "gaussian_gen = SyntheticDataGenerator(scale=0.8)\n",
        "s_gen = SyntheticDataGenerator(datasets.make_s_curve, scale=2)\n",
        "swiss_gen = SyntheticDataGenerator(datasets.make_swiss_roll, scale=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "t_ahl1pNcolQ",
      "metadata": {
        "id": "t_ahl1pNcolQ"
      },
      "outputs": [],
      "source": [
        "X = gaussian_gen(500)\n",
        "Y_1 = s_gen(500)\n",
        "Y_2 = swiss_gen(500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ezuusYJJq3PJ",
      "metadata": {
        "id": "ezuusYJJq3PJ"
      },
      "source": [
        "We compute the transportation maps using our estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "UDzY7cXPczGD",
      "metadata": {
        "id": "UDzY7cXPczGD"
      },
      "outputs": [],
      "source": [
        "s_curve_map = get_estimated_transport_map(X, Y_1, epsilon=0.075)\n",
        "swiss_roll_map = get_estimated_transport_map(X, Y_2, epsilon=0.075)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WyA8VYo1q8wl",
      "metadata": {
        "id": "WyA8VYo1q8wl"
      },
      "source": [
        "And plot them, displaying the matching between the source and the transported source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2Nmh-BcthXKu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "2Nmh-BcthXKu",
        "outputId": "0e6d085b-c9a4-4fa4-9890-a207c6b882c5"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "Y_1_transported = s_curve_map(X)\n",
        "\n",
        "ax1.scatter(X[:, 0], X[:, 1], label=\"source\")\n",
        "ax1.scatter(Y_1[:, 0], Y_1[:, 1], label=\"target\")\n",
        "ax1.scatter(Y_1_transported[:, 0], Y_1_transported[:, 1], label=\"Transported source\")\n",
        "ax1.legend()\n",
        "\n",
        "\n",
        "for p1, p2 in zip(X, Y_1_transported):\n",
        "    ax1.plot([p1[0], p2[0]], [p1[1], p2[1]], \"k-\", alpha=0.2)\n",
        "\n",
        "Y_2_transported = swiss_roll_map(X)\n",
        "\n",
        "ax2.scatter(X[:, 0], X[:, 1], label=\"source\")\n",
        "ax2.scatter(Y_2[:, 0], Y_2[:, 1], label=\"target\")\n",
        "ax2.scatter(Y_2_transported[:, 0], Y_2_transported[:, 1], label=\"Transported source\")\n",
        "ax2.legend()\n",
        "\n",
        "for p1, p2 in zip(X, Y_2_transported):\n",
        "    ax2.plot([p1[0], p2[0]], [p1[1], p2[1]], \"k-\", alpha=0.2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eBwXuaXapjig",
      "metadata": {
        "id": "eBwXuaXapjig"
      },
      "source": [
        "As we can see, the vast majority of the matchings do not cross, which is encouraging for the quality of our estimated map. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
